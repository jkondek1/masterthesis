{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Jakub/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Jakub/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/Jakub/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#downloading and installing needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re #regular expressions\n",
    "import nltk #natural language processing \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Data\n",
    "texts = pd.read_csv(\"quality_for_ai_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass    5419\n",
      "Fail     405\n",
      "Name: EMPATHY_RECODED, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#selecting only important variables of the dataset\n",
    "texts = texts[[\"MSG_BODY\",\"EMPATHY_RECODED\"]]\n",
    "\n",
    "#changing variable empathy_recoded into categorical variable\n",
    "texts[\"EMPATHY_RECODED\"] = texts[\"EMPATHY_RECODED\"].astype(\"category\")\n",
    "\n",
    "#Droping all rows with missing observations (after we found there are NAs in EMPATHY_RECODED)\n",
    "texts.dropna(inplace = True)\n",
    "texts.reset_index(inplace = True,drop = True)\n",
    "#texts = texts.dropna()\n",
    "#counting number of observ. for each factor\n",
    "print(texts[\"EMPATHY_RECODED\"].value_counts())\n",
    "#only 7 percent fails\n",
    "\n",
    "#storing modified datafile\n",
    "texts_or = texts.copy() #musi byt copy, lebo je to inak len pointer!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_var = texts[\"MSG_BODY\"].copy()\n",
    "\n",
    "#reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for text in range(0,len(texts_var)):\n",
    "    texts_var.iloc[text] = texts_var.iloc[text].lower()\n",
    "    texts_var.iloc[text] = re.sub(r'[\\n\\xa0]','',str(texts_var.iloc[text])) #get rid of all \\n and \\xa0 \n",
    "    #first is dear, then normally there's mr./mrs. or sthm like that and continues with thanking and possibly kiwi.com\n",
    "    texts_var.iloc[text] = re.sub(r'dear[\\w\\s]*\\.*[\\w\\s]*,*[\\w\\s]*[\\.com]*\\.*','',str(texts_var.iloc[text])) #look how to shorten the regex\n",
    "    #last sentence (or potentially last two are always kind regards and name)\n",
    "    texts_var.iloc[text] = re.sub(r'\\.[\\w\\s,]*\\.?[\\w\\s,]*travel consultant[\\w\\s\\.]*','.',str(texts_var.iloc[text]))\n",
    "    \n",
    "    #Remove all the special characters  ... zatial nepouzivam lebo neviem co s bodkami\n",
    "    #texts_var.iloc[text] = re.sub(r'\\W', ' ', str(texts_var.iloc[text]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    texts_var.iloc[text] = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', str(texts_var.iloc[text]))\n",
    "    # Substituting multiple spaces with single space\n",
    "    texts_var.iloc[text]= re.sub(r'\\s+', ' ', str(texts_var.iloc[text]), flags=re.I)\n",
    "    \n",
    "    #najpv zrus emailove adresy, hyperlinky, kiwi.com, ()\n",
    "    texts_var.iloc[text] = re.sub(r'\\w*?.?\\w*@','', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'https://.*manage/','', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'kiwi.com','', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'\\(.*?\\)','', str(texts_var.iloc[text]))\n",
    "    \n",
    "    #remove all remainig \",\",\".\",etc.\n",
    "    texts_var.iloc[text] = re.sub(r'([^\\s\\w]|_)+', ' ', str(texts_var.iloc[text]))\n",
    "    \n",
    "    #following is deletion of digits... currently not being used\n",
    "    #texts_var.iloc[text] = re.sub(r'[\\d]+', ' ', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'[\\s\\s]+', ' ', str(texts_var.iloc[text]))\n",
    "    \n",
    "    #randomly found problems - solved \n",
    "    #dividing of e-ticket\n",
    "    texts_var.iloc[text] = re.sub(r'e ticket', 'eticket', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'e mail', 'email', str(texts_var.iloc[text]))\n",
    "    \n",
    "    #Following is tried to help the performance of final model\n",
    "    #20 eur issue, etc.\n",
    "    texts_var.iloc[text] = re.sub(r'20 eur', '20eur', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'20 euro', '20eur',str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'24 hours', '24hours', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'48 hours', '48hours', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'30 days', '30days', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'24 7', 'nonstop', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'5 star', '5star', str(texts_var.iloc[text]))\n",
    "    texts_var.iloc[text] = re.sub(r'12 24hours', '12to24hours', str(texts_var.iloc[text])) #24hours bcs it is already modified above\n",
    "    texts_var.iloc[text] = re.sub(r'36 hours', '36hours', str(texts_var.iloc[text]))\n",
    "    \n",
    "    #Stopwords (a, the, personal pronouns...) moc sa mi to nezda lebo sa tam stracaju nejake vyznamove prejenia\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    texts_var.iloc[text] = word_tokenize(texts_var.iloc[text]) \n",
    "    filtered_sentences = [w for w in texts_var.iloc[text] if not w in stop_words] \n",
    "    texts_var.iloc[text] = []\n",
    "    for w in filtered_sentences: \n",
    "        if w not in stop_words: \n",
    "            texts_var.iloc[text].append(w) \n",
    "    \n",
    "#lemmatizer gives totally bad results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words is 904\n",
      "The max number of words has word number 5297\n"
     ]
    }
   ],
   "source": [
    "# how many words have texts max\n",
    "max_memory = 1\n",
    "index = 1\n",
    "lengths = [0] * len(texts_var)\n",
    "for i in range(len(texts_var)):\n",
    "    lengths[i] = len(texts_var[i])\n",
    "    if lengths[i] > max_memory:\n",
    "        max_memory = lengths[i]\n",
    "        index = i\n",
    "print(\"Max number of words is\", max_memory)\n",
    "print(\"The max number of words has word number\", index)\n",
    "\n",
    "pd.DataFrame(lengths).describe()\n",
    "\n",
    "#deleting too long texts ... mostly texts in different language through automatic translations \n",
    "\n",
    "#saving dataframe of word lengths\n",
    "lengths = pd.DataFrame(lengths)\n",
    "\n",
    "#naming the only variable in the dataframe\n",
    "lengths.columns = ['words']\n",
    "\n",
    "#filtering only observations less than ...\n",
    "indexes = lengths.loc[lengths['words'] < 300].index.values \n",
    "\n",
    "#updating texts_var\n",
    "texts_or = texts_or.iloc[indexes,:]\n",
    "texts_var = texts_var[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting all fail empathy texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"fail_texts.txt\",\"w\")\n",
    "fail_texts_indexes = texts_or[texts_or[\"EMPATHY_RECODED\"]==\"Fail\"].index.values\n",
    "for i in fail_texts_indexes:\n",
    "    file.write(texts_or.iloc[i,0])\n",
    "file.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/envs/master2/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for tokenizing\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#assign numbers to each word from texts (max 5000)\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts_var)\n",
    "\n",
    "#split the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(texts_var, train_size = 0.7)\n",
    "\n",
    "#tokenize train and test (lists are created)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train)\n",
    "X_test = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#>>> print(sentences_train[2])\n",
    "#>>> print(X_train[2])\n",
    "#Of all the dishes, the salmon was the best, but all were great.\n",
    "#[11, 43, 1, 171, 1, 283, 3, 1, 47, 26, 43, 24, 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting all the files used in further modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#train.to_csv(\"train.csv\",header = False,quoting = csv.QUOTE_NONNUMERIC)\n",
    "#test.to_csv(\"test.csv\",header = False,quoting = csv.QUOTE_NONNUMERIC)\n",
    "#texts_or.to_csv(\"texts_or_shorter.csv\",header = True,quoting = csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "train.to_pickle(\"train2.pkl\")\n",
    "test.to_pickle(\"test2.pkl\")\n",
    "texts_or.to_pickle(\"texts_or_shorter2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_or.iloc[1295,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_var[1295]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4810    Pass\n",
       "4613    Fail\n",
       "2478    Pass\n",
       "5059    Pass\n",
       "4137    Pass\n",
       "Name: EMPATHY_RECODED, dtype: category\n",
       "Categories (2, object): [Fail, Pass]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create labels to the test and train set\n",
    "Y_train = texts_or[\"EMPATHY_RECODED\"][train.index]\n",
    "\n",
    "Y_test = texts_or[\"EMPATHY_RECODED\"][test.index]\n",
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)+len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out how distributed is empathy pass/fail in the train/test sample\n",
    "\n",
    "#save train as dataframe\n",
    "train = pd.DataFrame(train)\n",
    "\n",
    "#store the original indexes from texts_or as a variable\n",
    "train.reset_index(inplace = True)\n",
    "\n",
    "#select only the indexes of train and count the #of observ. for each level\n",
    "texts_or[\"EMPATHY_RECODED\"][train.index].value_counts()\n",
    "\n",
    "\n",
    "##the distribution is similar to the whole sample, analogically we tested the test file - same applies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "125/1856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting maximum length and padding of tokenized texts (texts in form of numbers)\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 100 #originally 300\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Help functions - tokenized corpus\n",
    "\n",
    "#tokenizer.word_index\n",
    "#print('{}: {}'.format('pocet', tokenizer.word_index[\"flight\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxiliary functions\n",
    "\n",
    "#checking how many sentences with thank you there are\n",
    "x = 0\n",
    "without_thankyou = []\n",
    "for text in range(0,len(texts_var)):\n",
    "    if re.search(r'thank you',str(texts_var.iloc[text])) != None :\n",
    "        x = x+1\n",
    "    else:\n",
    "        if re.search(r'thanks',str(texts_var.iloc[text])) != None :\n",
    "            x = x+1\n",
    "        else:\n",
    "            without_thankyou.append(text) \n",
    "\n",
    "print(len(without_thankyou)/len(texts_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "storage_firstlines = []\n",
    "#Check of all the first lines... whether the original regex is correct... seems ok\n",
    "for text in range(0,len(texts_emphatic)):\n",
    "    pattern = re.compile(r'[\\w\\s]*\\.')\n",
    "    storage_firstlines.append(pattern.search(texts_emphatic.iloc[text]))\n",
    "    #search does not do what i wanted (showing the whole first lines), but serves the purpose (incorrect regex probably)\n",
    "storage_firstlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need Y_train and Y_test as binaries\n",
    "#first create dictionary for Pass and FAil\n",
    "convert = {\"Pass\":1, \"Fail\":0}\n",
    "#now use method replace\n",
    "Y_train.replace(convert,inplace = True)\n",
    "Y_test.replace(convert,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 900)               90900     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               270300    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 361,802\n",
      "Trainable params: 361,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 4014 samples, validate on 1721 samples\n",
      "Epoch 1/1000\n",
      "4014/4014 [==============================] - 10s 3ms/step - loss: 1.1203 - acc: 0.9305 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 2/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 3/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 4/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 5/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 6/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 7/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 8/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 9/1000\n",
      "4014/4014 [==============================] - 9s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 10/1000\n",
      "4014/4014 [==============================] - 9s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 11/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 12/1000\n",
      "4014/4014 [==============================] - 8s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 13/1000\n",
      "4014/4014 [==============================] - 9s 2ms/step - loss: 1.1002 - acc: 0.9317 - val_loss: 1.1707 - val_acc: 0.9274\n",
      "Epoch 14/1000\n",
      " 710/4014 [====>.........................] - ETA: 7s - loss: 0.9989 - acc: 0.938"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f48792baecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                    callbacks=[EarlyStopping(patience = 15, #restore_best_weights=True can be added\n\u001b[0;32m---> 29\u001b[0;31m                                       monitor='acc')]\n\u001b[0m\u001b[1;32m     30\u001b[0m                    )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mprev_total_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\b'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History \n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "Y_test_bin = pd.DataFrame(to_categorical(Y_test))\n",
    "Y_train_bin = pd.DataFrame(to_categorical(Y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(900, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(300, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, Y_train_bin,\n",
    "                   batch_size = 10,\n",
    "                   epochs=1000,\n",
    "                   verbose=True,\n",
    "                   validation_data=(X_test, Y_test_bin),\n",
    "                   callbacks=[EarlyStopping(patience = 15, #restore_best_weights=True can be added\n",
    "                                      monitor='acc')]\n",
    "                   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748\n",
      "0\n",
      "1625\n"
     ]
    }
   ],
   "source": [
    "#Acc is around 93% which is however cause by unbalanced dataset --> all the targets are assigned 1s\n",
    "#to check\n",
    "#predict X_test once again\n",
    "predictions = model.predict_classes(X_test)\n",
    "\n",
    "#number of predicted observations\n",
    "print(len(predictions))\n",
    "#sum of predicted observations\n",
    "print(sum(predictions))\n",
    "#sum of original targets\n",
    "print(sum(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the problem of unbalanced dataset (with oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_test_bin = pd.DataFrame(to_categorical(Y_test))\n",
    "Y_train_bin = pd.DataFrame(to_categorical(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load function resample, which we use\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenate our training data back together\n",
    "Y_train.reset_index(inplace = True,drop = True)\n",
    "X_train_full = pd.DataFrame(X_train)\n",
    "X_train_full[\"EMPATHY_RECODED\"] = Y_train\n",
    "\n",
    "#separate minority and majority classes\n",
    "emp_pass = X_train_full[X_train_full.EMPATHY_RECODED == 1]\n",
    "emp_fail = X_train_full[X_train_full.EMPATHY_RECODED == 0]\n",
    "\n",
    "# upsample minority\n",
    "emp_fail_upsampled = resample(emp_fail,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(emp_pass), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([emp_pass, emp_fail_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "print(upsampled.EMPATHY_RECODED.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.DataFrame(to_categorical(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50000)             5050000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              50001000  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 55,053,002\n",
      "Trainable params: 55,053,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Applications/Anaconda/anaconda3/envs/master/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5668 samples, validate on 1322 samples\n",
      "Epoch 1/1000\n",
      "5668/5668 [==============================] - 650s 115ms/step - loss: 8.0517 - acc: 0.5002 - val_loss: 1.0729 - val_acc: 0.9334\n",
      "Epoch 2/1000\n",
      "5668/5668 [==============================] - 610s 108ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 1.0729 - val_acc: 0.9334\n",
      "Epoch 3/1000\n",
      " 740/5668 [==>...........................] - ETA: 8:23 - loss: 7.8412 - acc: 0.5135"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History \n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "Y_train = upsampled.EMPATHY_RECODED\n",
    "X_train = upsampled.drop('EMPATHY_RECODED', axis=1)\n",
    "Y_train = pd.DataFrame(to_categorical(Y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(50000, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1000, input_dim=input_dim, activation='relu'))\n",
    "#model.add(layers.Dense(50, input_dim=input_dim, activation='relu'))\n",
    "#model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                   batch_size = 20,\n",
    "                   epochs=1000,\n",
    "                   verbose=True,\n",
    "                   validation_data=(X_test, Y_test_bin),\n",
    "                   callbacks=[EarlyStopping(patience = 5, #restore_best_weights=True can be added\n",
    "                                      monitor='acc')]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374\n",
      "1374\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "#Acc is around 93% which is however cause by unbalanced dataset --> all the targets are assigned 1s\n",
    "#to check\n",
    "#predict X_test once again\n",
    "predictions2 = model.predict_classes(X_test)\n",
    "\n",
    "#number of predicted observations\n",
    "print(len(predictions2))\n",
    "#sum of predicted observations\n",
    "print(sum(predictions2))\n",
    "#sum of original targets\n",
    "print(sum(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
