{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Dropout,Flatten, MaxPooling2D, Dense, Softmax, Input, concatenate\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "#from tensorflow.random import set_seed\n",
    "#set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "pos = np.load(\"../processed_final_texts/pos_lemma.npy\")\n",
    "neg = np.load(\"../processed_final_texts/neg_lemma.npy\")\n",
    "data = np.concatenate([pos,neg])\n",
    "targets = np.array(list([1] * 719))#([20194,1])\n",
    "targets[595:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_array = pd.read_pickle(\"../processed_final_texts/_lemma.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traditional Kim 2014 (CNN)\n",
    "def create_CNN():\n",
    "    \n",
    "    input_shape = Input(shape=(397, 50, 1))\n",
    "        \n",
    "    f_1 = Conv2D(100,kernel_size = (3,50))(input_shape)\n",
    "    f_1 = MaxPooling2D((395, 1), strides=(1, 1), padding='same')(f_1)\n",
    "\n",
    "    f_2 = Conv2D(100,kernel_size = (4,50))(input_shape)\n",
    "    f_2 = MaxPooling2D((394, 1), strides=(1, 1), padding='same')(f_2)\n",
    "    \n",
    "    f_3 = Conv2D(100,kernel_size = (5,50))(input_shape)\n",
    "    f_3 = MaxPooling2D((393, 1), strides=(1, 1), padding='same')(f_3)\n",
    "\n",
    "    merged = concatenate([f_1, f_2, f_3], axis=1)\n",
    "    merged = Flatten()(merged)\n",
    "\n",
    "    out = Dense(100, activation='relu')(merged)\n",
    "    out = Dense(2, activation='softmax')(out)\n",
    "\n",
    "    model = Model(input_shape, out)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def process_embedding_data(input_data):\n",
    "    from statistics import mean\n",
    "    values = np.zeros([input_data.shape[0],3])\n",
    "    for obs in range(input_data.shape[0]):\n",
    "        #need to check how exactly the max/min/mean are done\n",
    "        values[obs,0] = max(input_data[obs].ravel())\n",
    "        values[obs,1] = min(input_data[obs].ravel())\n",
    "        values[obs,2] = mean(list(input_data[obs].ravel()))\n",
    "    \n",
    "    return values\n",
    "\n",
    "\n",
    "\n",
    "def KNN_transform(input_data,aggregate_embedding = False):   \n",
    "    if aggregate_embedding == True:\n",
    "        values = process_embedding_data(input_data)       \n",
    "    else:\n",
    "        values = input_data.reshape([input_data.shape[0],19850])\n",
    "        \n",
    "    return values\n",
    "\n",
    "def tokenize_texts(x_train,x_test,n_words = 1000):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df = 3, max_features = n_words)\n",
    "    x_train_tfidf = vectorizer.fit_transform(x_train).toarray()\n",
    "    x_test_tfidf = vectorizer.transform(x_test).toarray()\n",
    "\n",
    "    return x_train_tfidf, x_test_tfidf\n",
    "\n",
    "def CNN(x_train,x_test,y_train,y_test):\n",
    "    opt = SGD(learning_rate=0.0001)\n",
    "    x_train = x_train.reshape([x_train.shape[0],397,50,1])\n",
    "    x_test = x_test.reshape([x_test.shape[0],397,50,1])\n",
    "    y_train = to_categorical(y_train)\n",
    "    #print(\"train value\", y_train)\n",
    "    #print(\"test value\", y_test)\n",
    "    model = create_CNN()\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, batch_size = len(x_train), epochs=30, verbose=2, validation_split = 0.0,\n",
    "          callbacks = [EarlyStopping(monitor=\"accuracy\",patience = 3, restore_best_weights=True)])\n",
    "    predicted = model.predict(x_test).argmax(axis = 1)\n",
    "    #print(\"predicted value\" ,predicted)\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    return accuracy, y_test.item(), predicted.item()\n",
    "\n",
    "def SVM(x_train,x_test,y_train,y_test, tfidf):\n",
    "    \n",
    "    #tfidf parameter is prepared for diffentiatig of SVM parameters in case of tfidf data and embedding data\n",
    "    if tfidf:\n",
    "        x_train, x_test = tokenize_texts(x_train,x_test)\n",
    "    else:\n",
    "        x_train = process_embedding_data(x_train)\n",
    "        x_test = process_embedding_data(x_test)\n",
    "    \n",
    "    SVM = LinearSVC(random_state = 0)\n",
    "    SVM.fit(x_train,y_train)\n",
    "    predicted = SVM.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test,predicted)\n",
    "    print(\"y_test\",y_test,\"predicted\",predicted,\"accuracy\",accuracy)\n",
    "    print(y_test, y_test.item())\n",
    "    return accuracy, y_test.item(), predicted.item()\n",
    "\n",
    "def KNN(x_train,x_test,y_train,y_test):\n",
    "    x_train = KNN_transform(x_train,True)\n",
    "    x_test = KNN_transform(x_test,True)\n",
    "    KNN = KNeighborsClassifier()\n",
    "    KNN.fit(x_train,y_train)\n",
    "    predicted = KNN.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test,predicted)\n",
    "    print(\"y_test\",y_test,\"predicted\",predicted,\"accuracy\",accuracy)\n",
    "    return accuracy, y_test.item(), predicted.item()\n",
    "\n",
    "def NB(x_train,x_test,y_train,y_test,tfidf = True):\n",
    "    x_train, x_test = tokenize_texts(x_train,x_test)\n",
    "    naive = MultinomialNB()\n",
    "    classifier = naive.fit(x_train,y_train)\n",
    "    predicted = classifier.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test,predicted)\n",
    "    return accuracy, y_test.item(), predicted.item()\n",
    "\n",
    "##\n",
    "def choose_model(x_train,x_test,y_train,y_test,model,tfidf = False):\n",
    "    if model == \"CNN\":\n",
    "        return CNN(x_train,x_test,y_train,y_test)\n",
    "    elif model == \"SVM\":\n",
    "        return SVM(x_train,x_test,y_train,y_test,tfidf = tfidf)\n",
    "    elif model == \"KNN\":\n",
    "        return KNN(x_train,x_test,y_train,y_test)\n",
    "    elif model == \"NB\":\n",
    "        return NB(x_train,x_test,y_train,y_test,tfidf = tfidf)\n",
    "    else:\n",
    "        print(\"model not known\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function for leave-one-out testing\n",
    "def robust_accuracy(data,targets,model,texts = None,upsample = False, tfidf = False):\n",
    "    import statistics as stat\n",
    "    all_accuracies = []\n",
    "    actual_list = []\n",
    "    predicted_list = []\n",
    "    crossVal = LeaveOneOut()\n",
    "    \n",
    "    if texts:\n",
    "        data = np.array(texts)\n",
    "    \n",
    "    for train_index, test_index in crossVal.split(data):\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        x_train, x_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = targets[train_index], targets[test_index]\n",
    "        if upsample == True:\n",
    "            x_to_add = x_train[y_train == 0]\n",
    "            y_to_add = y_train[y_train == 0]\n",
    "            for i in range(0,2):\n",
    "                x_train = np.append(x_train,x_to_add,axis = 0)\n",
    "                y_train = np.append(y_train,y_to_add,axis = 0)\n",
    "        #print(x_train.shape,\" \",x_test.shape,\" \",y_train.shape,\" \",y_test.shape)\n",
    "        if texts:\n",
    "            x_train = x_train.tolist()\n",
    "            x_test = x_test.tolist()\n",
    "        accuracy, actual, predicted = choose_model(x_train,x_test,y_train,y_test,model,tfidf = tfidf)\n",
    "        \n",
    "        all_accuracies.append(accuracy)\n",
    "        test_accuracy = stat.mean(all_accuracies)\n",
    "        actual_list.append(actual)\n",
    "        predicted_list.append(predicted)\n",
    "        print(\"index\",test_index,\"accuracy\",accuracy)\n",
    "    return test_accuracy, all_accuracies, actual_list, predicted_list\n",
    "\n",
    "results = robust_accuracy(data,\n",
    "                          targets,\n",
    "                          #texts = pd.read_pickle(\"../preprocessed_texts_lemma.pkl\"), \n",
    "                          model = \"KNN\",\n",
    "                          tfidf = False,\n",
    "                          upsample = False)\n",
    "\n",
    "print(classification_report(results[2], results[3], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results,open(\"../results/bech_res_KNN_lemma_sent.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = robust_accuracy(data,\n",
    "                          targets,\n",
    "                          texts = pd.read_pickle(\"../preprocessed_texts_lemma.pkl\"), \n",
    "                          model = \"SVM\",\n",
    "                          tfidf = True,\n",
    "                          upsample = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results1,open(\"../results/SVM1000_lemma_upsample.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
